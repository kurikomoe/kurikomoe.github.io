{"title":"くりこの強化学習ノート","author":"Kuriko Moe (栗子Chan)","excerpt":"<p>这里记载了くりこ的强化学习笔记（真的是复习笔记）</p>\n","render_content":"<p>这里记载了くりこ的强化学习笔记（真的是复习笔记）</p>\n<!-- more -->\n<h1 id=\"key-concepts\" tabindex=\"-1\">Key Concepts</h1>\n<blockquote>\n<p>参考：<a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\" target=\"_blank\" rel=\"noopener\">https://spinningup.openai.com/en/latest/spinningup/rl_intro.html</a></p>\n</blockquote>\n<ol>\n<li>\n<p>states and observation: 通常记作 state: \\(s\\),  observation: \\(o\\)</p>\n</li>\n<li>\n<p>action space</p>\n</li>\n<li>\n<p>policies\n确定性策略：\\(a_t = \\mu (s_t)\\)\n随机策略：\\(a_t \\sim \\pi(\\cdot | s_t)\\)\n当策略依赖于参数时，记作：\\(a_t = \\mu_\\theta(s_t), a_t \\sim \\pi_\\theta(\\cdot | s_t)\\)</p>\n</li>\n<li>\n<p>确定性(deterministic)策略：某个状态 \\(s\\) 的策略应当是固定的，其概率应该始终为1，不会发生改变</p>\n</li>\n<li>\n<p>随机(stochastic)策略：从状态集 \\(S\\) 到动作集 \\(A\\) 的条件概率分布，即 \\(\\pi_S(A|S)\\) 动作是从分布中 sample 得到的</p>\n<blockquote>\n<p>参考：<a href=\"https://zhuanlan.zhihu.com/p/136020868\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/136020868</a></p>\n</blockquote>\n<p>随机策略里面两个比较重要的计算：</p>\n<ol>\n<li>sampling actions from the policy</li>\n<li>and computing log likelihoods of particualr actions: \\(log\\pi_\\theta(a|s)\\)</li>\n</ol>\n</li>\n<li>\n<p>Trajectories: a sequence of states and actions in the world, noted as \\(\\tau\\), \\(\\tau = (s_0, a_0, s_1, a_1, \\dots)\\)</p>\n<p>初始状态一般是从 start-state distribution 中采样得到： \\(s_0 \\sim p_0(\\cdot)\\)</p>\n<p>状态转移(State transitions): \\(s_{t+1} = f(s_t, a_t)\\) 或者 \\(s_{t+1} \\sim P(\\cdot | s_t, a_t)\\)</p>\n<blockquote>\n<p>Trajectories are also frequently called episodes or rollouts</p>\n</blockquote>\n</li>\n<li>\n<p>Reward and Return \\(r_t = R(s_t, a_t, s_{t+1})\\)\n引入折扣因子(discount factor) \\(\\gamma \\in (0, 1)\\), \\(R(\\tau) = \\sum_{t=0}^{\\infty}{\\gamma^t r_t}\\)。</p>\n</li>\n</ol>\n<h2 id=\"rl-problem\" tabindex=\"-1\">RL Problem</h2>\n<blockquote>\n<p>the goal in RL is to select</p>\n<p>a policy which maximizes expected return when the agent acts according to it.l</p>\n</blockquote>\n<p>在随机策略下，一个 \\(\\tau\\)-steps 的 trajectory 的概率是</p>\n<p>\\[\n\\begin{align*}\nP(\\tau | \\pi) &amp;= \\rho_0(s_0) \\Pi_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t) \\\\\\\\\nJ(\\pi) &amp;= \\int_\\tau P(\\tau | \\pi) R(\\tau) = E_{\\tau \\sim \\pi} [R(\\tau) \\\\\\\\\n\\pi^* &amp;=  \\arg \\max_\\pi J(\\pi)\n\\end{align*}\n\\]</p>\n<h3 id=\"value-functions%EF%BC%88%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%89\" tabindex=\"-1\">Value Functions（值函数）</h3>\n<h3 id=\"bellman-equation\" tabindex=\"-1\">Bellman Equation</h3>\n<blockquote>\n<p>The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.</p>\n</blockquote>\n","toc":[{"content":"Key Concepts","slug":"key-concepts","lvl":1,"i":0,"seen":0},{"content":"RL Problem","slug":"rl-problem","lvl":2,"i":1,"seen":0},{"content":"Value Functions（值函数）","slug":"value-functions%EF%BC%88%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%89","lvl":3,"i":2,"seen":0},{"content":"Bellman Equation","slug":"bellman-equation","lvl":3,"i":3,"seen":0}],"slug":"学习笔记/科研/rl","published":true,"date":"2021-03-02T03:18:52.000Z","updated":"2021-03-02T07:51:53.000Z","photos":[],"link":"","path":"学习笔记/科研/rl/","permalink":"https://kuriko.moe/2021/03/02/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%A7%91%E7%A0%94/rl/","api_path":"api/articles/学习笔记/科研/rl.json","keywords":null,"cover":null,"categories":[],"tags":[{"name":"学习笔记","path":"api/tags/学习笔记.json"},{"name":"深度学习","path":"api/tags/深度学习.json"}]}